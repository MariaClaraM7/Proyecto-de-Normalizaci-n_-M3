{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Descarga del Conjunto de Datos:\n",
        "\n",
        "*  Accede al conjunto de datos en [Kaggle](https://www.kaggle.com/datasets/datafiniti/consumer-reviews-of-amazon-products).\n",
        "*  Inicia sesión en tu cuenta de Kaggle y descarga el archivo CSV correspondiente."
      ],
      "metadata": {
        "id": "YQAVFxIkf6Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Carga del Conjunto de Datos en Jupyter Notebook:\n",
        "\n",
        "*  Utiliza la biblioteca **pandas** para cargar el archivo CSV en un DataFrame"
      ],
      "metadata": {
        "id": "T756dalxgRm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SVQaEyePfw_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "690c7e7a-760d-446f-b42c-bb1c5e5852be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4384c6cf98c4>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Cargar el conjunto de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Mostrar las primeras filas del DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "df = pd.read_csv('/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv')\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tokenización:\n",
        "\n",
        "Emplea nltk para dividir el texto en tokens:"
      ],
      "metadata": {
        "id": "qR6C5QXpgcrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "# Descargar recursos de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Si sale un error\n",
        "nltk.download('punkt_tab')\n",
        "# Tokenizar una reseña de ejemplo\n",
        "# Ejemplo de texto\n",
        "text = \"El cafe tiene un muy aroma y su color caracteristico representa una buen tostado.\"\n",
        "\n",
        "#Tokenización\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJfRzB2SgfOh",
        "outputId": "c0a7dd76-fed8-42de-d77f-18662f97d89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['El', 'cafe', 'tiene', 'un', 'muy', 'aroma', 'y', 'su', 'color', 'caracteristico', 'representa', 'una', 'buen', 'tostado', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Normalización:\n",
        "\n",
        "Convierte el texto a minúsculas y elimina caracteres especiales:"
      ],
      "metadata": {
        "id": "fhXxNLoJiJha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalización (conversión a minúsculas)\n",
        "tokens_normalized = [token.lower() for token in tokens]\n",
        "print(\"Tokens Normalizados:\", tokens_normalized)\n"
      ],
      "metadata": {
        "id": "m4K5fQaMgmhy",
        "outputId": "57d72629-88d8-4f9e-da46-fa519ed9e2b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Normalizados: ['el', 'cafe', 'tiene', 'un', 'muy', 'aroma', 'y', 'su', 'color', 'caracteristico', 'representa', 'una', 'buen', 'tostado', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Eliminación de Stopwords:\n",
        "\n",
        "Utiliza **nltk** para eliminar palabras comunes que no aportan valor semántico:"
      ],
      "metadata": {
        "id": "oCVyNA3sgowO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar lista de stopwords\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_no_stopwords = [token for token in tokens_normalized if token not in stop_words]\n",
        "print(\"Tokens sin Stopwords:\", tokens_no_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH0T1AQvgt9E",
        "outputId": "b5d6d7b3-8508-49ec-be37-a31dec46e354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens sin Stopwords: ['cafe', 'aroma', 'color', 'caracteristico', 'representa', 'buen', 'tostado', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Lematización:\n",
        "\n",
        "Aplica WordNetLemmatizer de nltk para reducir las palabras a su forma base:"
      ],
      "metadata": {
        "id": "2bru-gvjgxbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens_lemmatized = [lemmatizer.lemmatize(token) for token in tokens_no_stopwords]\n",
        "print(\"Tokens Lematizados:\", tokens_lemmatized)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3jrxlGEgvxU",
        "outputId": "aefbc08f-c0aa-4141-9060-2fa22b804213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Lematizados: ['cafe', 'aroma', 'color', 'caracteristico', 'representa', 'buen', 'tostado', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Stemming:\n",
        "\n",
        "Alternativamente, utiliza **PorterStemmer** para reducir las palabras a su raíz:"
      ],
      "metadata": {
        "id": "Nl_a-G9rg39k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "tokens_stemmed = [stemmer.stem(token) for token in tokens_no_stopwords]\n",
        "print(\"Tokens Stemmed:\", tokens_stemmed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MWZN_o3Bg7iX",
        "outputId": "3d1e8f58-4f9b-44b7-b516-a1f2830d90d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Stemmed: ['cafe', 'aroma', 'color', 'caracteristico', 'representa', 'buen', 'tostado', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Representación de Texto:\n",
        "\n",
        "*  Bolsa de Palabras (Bag of Words):\n",
        "\n",
        "  *  Utiliza CountVectorizer de **sklearn**:"
      ],
      "metadata": {
        "id": "pBJ8wereg-gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Representación de Texto\n",
        "\n",
        "# Bolsa de Palabras\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform([' '.join(tokens_no_stopwords)])\n",
        "print(\"\\nBolsa de Palabras:\")\n",
        "print(X_bow.toarray())\n",
        "print(\"Características (BOW):\", vectorizer_bow.get_feature_names_out())"
      ],
      "metadata": {
        "id": "kleTUVaPhFSI",
        "outputId": "7ddcee2f-0af9-468a-8de1-b490914a6583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bolsa de Palabras:\n",
            "[[1 1 1 1 1 1 1]]\n",
            "Características (BOW): ['aroma' 'buen' 'cafe' 'caracteristico' 'color' 'representa' 'tostado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  TF-IDF:\n",
        "\n",
        "  *  Aplica TfidfVectorizer de sklearn:"
      ],
      "metadata": {
        "id": "rUpbp3NVhH-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform([' '.join(tokens_no_stopwords)])\n",
        "print(\"\\nTF-IDF:\")\n",
        "print(X_tfidf.toarray())\n",
        "print(\"Características (TF-IDF):\", vectorizer_tfidf.get_feature_names_out())\n",
        "\n"
      ],
      "metadata": {
        "id": "QYnAbwPohL0c",
        "outputId": "74b4d6f8-dd65-460a-9e3a-4cabf8f8fc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF:\n",
            "[[0.37796447 0.37796447 0.37796447 0.37796447 0.37796447 0.37796447\n",
            "  0.37796447]]\n",
            "Características (TF-IDF): ['aroma' 'buen' 'cafe' 'caracteristico' 'color' 'representa' 'tostado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Estandarización exitosa: El texto fue limpiado, tokenizado y transformado adecuadamente, permitiendo un análisis más profundo.\n",
        "#Flexibilidad para análisis: La implementación de técnicas como TF-IDF facilita la extracción de características clave del texto.\n",
        "#Base sólida para modelos: El conjunto de datos está listo para ser utilizado en tareas de clasificación, agrupamiento u otras aplicaciones."
      ],
      "metadata": {
        "id": "IZ9QekW30dhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}